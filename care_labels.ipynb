{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "care_labels.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMlXJ2yIV8e7",
        "colab_type": "text"
      },
      "source": [
        "## Choosing a pre training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3_Ns54i3HgO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "57df11f7-2f39-4fd2-e26d-332b2062994c"
      },
      "source": [
        "\n",
        "%tensorflow_version 1.x # Select module of the tensorflow\n",
        "# Some models to train on\n",
        "MODELS_CONFIG = {\n",
        "    'ssd_mobilenet_v2': {\n",
        "        'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\n",
        "        'pipeline_file': 'ssd_mobilenet_v2_coco.config',\n",
        "    }\n",
        "}\n",
        "\n",
        "selected_model = 'ssd_mobilenet_v2'\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.x # Select module of the tensorflow`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv3Zm042QGJy",
        "colab_type": "text"
      },
      "source": [
        "## Installing Required Packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68StUELaQPS2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "71da5c47-829f-4cee-d21d-b97f129844a9"
      },
      "source": [
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "\n",
        "!pip install -qq Cython contextlib2 pillow lxml matplotlib\n",
        "\n",
        "!pip install -qq pycocotools"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package python-bs4.\n",
            "(Reading database ... 144579 files and directories currently installed.)\n",
            "Preparing to unpack .../0-python-bs4_4.6.0-1_all.deb ...\n",
            "Unpacking python-bs4 (4.6.0-1) ...\n",
            "Selecting previously unselected package python-pkg-resources.\n",
            "Preparing to unpack .../1-python-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python-chardet.\n",
            "Preparing to unpack .../2-python-chardet_3.0.4-1_all.deb ...\n",
            "Unpacking python-chardet (3.0.4-1) ...\n",
            "Selecting previously unselected package python-six.\n",
            "Preparing to unpack .../3-python-six_1.11.0-2_all.deb ...\n",
            "Unpacking python-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python-webencodings.\n",
            "Preparing to unpack .../4-python-webencodings_0.5-2_all.deb ...\n",
            "Unpacking python-webencodings (0.5-2) ...\n",
            "Selecting previously unselected package python-html5lib.\n",
            "Preparing to unpack .../5-python-html5lib_0.999999999-1_all.deb ...\n",
            "Unpacking python-html5lib (0.999999999-1) ...\n",
            "Selecting previously unselected package python-lxml:amd64.\n",
            "Preparing to unpack .../6-python-lxml_4.2.1-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python-olefile.\n",
            "Preparing to unpack .../7-python-olefile_0.45.1-1_all.deb ...\n",
            "Unpacking python-olefile (0.45.1-1) ...\n",
            "Selecting previously unselected package python-pil:amd64.\n",
            "Preparing to unpack .../8-python-pil_5.1.0-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking python-pil:amd64 (5.1.0-1ubuntu0.3) ...\n",
            "Setting up python-pkg-resources (39.0.1-2) ...\n",
            "Setting up python-six (1.11.0-2) ...\n",
            "Setting up python-bs4 (4.6.0-1) ...\n",
            "Setting up python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Setting up python-olefile (0.45.1-1) ...\n",
            "Setting up python-pil:amd64 (5.1.0-1ubuntu0.3) ...\n",
            "Setting up python-webencodings (0.5-2) ...\n",
            "Setting up python-chardet (3.0.4-1) ...\n",
            "Setting up python-html5lib (0.999999999-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERyocH9U-o2Y",
        "colab_type": "text"
      },
      "source": [
        "## General imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEVLeKXh-s23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division, print_function, absolute_import\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import re\n",
        "import cv2 \n",
        "import os\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import io\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "from PIL import Image\n",
        "from collections import namedtuple, OrderedDict\n",
        "\n",
        "import shutil\n",
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "from google.colab import files"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8QeHvX6gpmC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b42d363-4fa8-4d93-cea0-cbc296bc1cbb"
      },
      "source": [
        "#tenorflow must be v1.15.2, because object detection API is removed from tf v 2.0+\n",
        "print(tf.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOcbTFEiPBKA",
        "colab_type": "text"
      },
      "source": [
        "## Downloading and Orgniazing Images and Annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebLK0eX_ezhJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "47ab4641-ba7e-4835-91ab-3c075f5ddd38"
      },
      "source": [
        "!git clone https://github.com/Dovahkiin169/CareLabelsData.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CareLabelsData'...\n",
            "remote: Enumerating objects: 144, done.\u001b[K\n",
            "remote: Counting objects: 100% (144/144), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 576 (delta 120), reused 124 (delta 100), pack-reused 432\u001b[K\n",
            "Receiving objects: 100% (576/576), 190.26 MiB | 6.83 MiB/s, done.\n",
            "Resolving deltas: 100% (332/332), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHPQQmhm7RLe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99874d13-c5c6-4551-c78e-e493e24fdff5"
      },
      "source": [
        "cd CareLabelsData"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/CareLabelsData\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JxCdR-c8yGp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOfuwfPrPSMz",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing Images and Labels\n",
        "1. Converting annotations from xml's to two csv files for `train_labels/` and `train_labels/`.\n",
        "2. Creating a label map file that specifies the number of class (one class in this case)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBHBFpWyEIDI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "43103c28-88fd-4ee3-c9a5-f5c8c9e522fa"
      },
      "source": [
        "%cd /content/CareLabelsData/data\n",
        "images_extension = 'jpg'\n",
        "\n",
        "def xml_to_csv(path):\n",
        "  classes_names = []\n",
        "  xml_list = []\n",
        "\n",
        "  for xml_file in glob.glob(path + '/*.xml'):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    for member in root.findall('object'):\n",
        "      classes_names.append(member[0].text)\n",
        "      value = (root.find('filename').text + '.' + images_extension,\n",
        "               int(root.find('size')[0].text),\n",
        "               int(root.find('size')[1].text),\n",
        "               member[0].text,\n",
        "               int(member[4][0].text),\n",
        "               int(member[4][1].text),\n",
        "               int(member[4][2].text),\n",
        "               int(member[4][3].text))\n",
        "      xml_list.append(value)\n",
        "  column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "  xml_df = pd.DataFrame(xml_list, columns=column_name) \n",
        "  classes_names = list(set(classes_names))\n",
        "  classes_names.sort()\n",
        "  return xml_df, classes_names\n",
        "\n",
        "for label_path in ['train_labels', 'test_labels']:\n",
        "  image_path = os.path.join(os.getcwd(), label_path)\n",
        "  xml_df, classes = xml_to_csv(label_path)\n",
        "  xml_df.to_csv(f'{label_path}.csv', index=None)\n",
        "  print(f'Successfully converted {label_path} xml to csv.')\n",
        "\n",
        "label_map_path = os.path.join(\"label_map.pbtxt\")\n",
        "\n",
        "pbtxt_content = \"\"\n",
        "\n",
        "for i, class_name in enumerate(classes):\n",
        "    pbtxt_content = (\n",
        "        pbtxt_content\n",
        "        + \"item {{\\n    id: {0}\\n    name: '{1}'\\n    display_name: '{1}'\\n }}\\n\\n\".format(i + 1, class_name)\n",
        "    )\n",
        "pbtxt_content = pbtxt_content.strip()\n",
        "with open(label_map_path, \"w\") as f:\n",
        "    f.write(pbtxt_content)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/CareLabelsData/data\n",
            "Successfully converted train_labels xml to csv.\n",
            "Successfully converted test_labels xml to csv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtfjZcD-CCdM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "52942f81-001e-400d-d034-589915539cdf"
      },
      "source": [
        "!cat label_map.pbtxt"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "item {\n",
            "    id: 1\n",
            "    name: 'bleaching_with_chlorine_allowed'\n",
            "    display_name: 'bleaching_with_chlorine_allowed'\n",
            " }\n",
            "\n",
            " item {\n",
            "    id: 2\n",
            "    name: 'chlorine_and_non_chlorine_bleach'\n",
            "    display_name: 'chlorine_and_non_chlorine_bleach'\n",
            " }\n",
            "\n",
            " item {\n",
            "    id: 3\n",
            "    name: 'do_not_bleach'\n",
            "    display_name: 'do_not_bleach'\n",
            " }\n",
            "\n",
            " item {\n",
            "    id: 4\n",
            "    name: 'do_not_dry_clean'\n",
            "    display_name: 'do_not_dry_clean'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 5\n",
            "    name: 'do_not_iron'\n",
            "    display_name: 'do_not_iron'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 6\n",
            "    name: 'do_not_tumble_drying'\n",
            "    display_name: 'do_not_tumble_drying'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 7\n",
            "    name: 'do_not_wash'\n",
            "    display_name: 'do_not_wash'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 8\n",
            "    name: 'do_not_wet_clean'\n",
            "    display_name: 'do_not_wet_clean'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 9\n",
            "    name: 'drip_dry_in_the_shade'\n",
            "    display_name: 'drip_dry_in_the_shade'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 10\n",
            "    name: 'drip_dry'\n",
            "    display_name: 'drip_dry'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 11\n",
            "    name: 'dry_clean_any_solvent'\n",
            "    display_name: 'dry_clean_any_solvent'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 12\n",
            "    name: 'dry_clean_hydrocarbon_solvent_only_hcs'\n",
            "    display_name: 'dry_clean_hydrocarbon_solvent_only_hcs'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 13\n",
            "    name: 'dry_clean_tetrachloroethylene_pce_only'\n",
            "    display_name: 'dry_clean_tetrachloroethylene_pce_only'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 14\n",
            "    name: 'dry_flat_in_the_shade'\n",
            "    display_name: 'dry_flat_in_the_shade'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 15\n",
            "    name: 'dry_flat'\n",
            "    display_name: 'dry_flat'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 16\n",
            "    name: 'dry_in_the_shade'\n",
            "    display_name: 'dry_in_the_shade'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 17\n",
            "    name: 'drying_symbol'\n",
            "    display_name: 'drying_symbol'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 18\n",
            "    name: 'gentle_cleaning_with_hydrocarbon_solvents'\n",
            "    display_name: 'gentle_cleaning_with_hydrocarbon_solvents'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 19\n",
            "    name: 'gentle_cleaning_with_pce'\n",
            "    display_name: 'gentle_cleaning_with_pce'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 20\n",
            "    name: 'gentle_wet_cleaning'\n",
            "    display_name: 'gentle_wet_cleaning'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 21\n",
            "    name: 'hand_wash'\n",
            "    display_name: 'hand_wash'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 22\n",
            "    name: 'ironing_at_high_temp'\n",
            "    display_name: 'ironing_at_high_temp'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 23\n",
            "    name: 'ironing_at_low_temp'\n",
            "    display_name: 'ironing_at_low_temp'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 24\n",
            "    name: 'ironing_at_med_temp'\n",
            "    display_name: 'ironing_at_med_temp'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 25\n",
            "    name: 'ironing'\n",
            "    display_name: 'ironing'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 26\n",
            "    name: 'line_dry_in_the_shade'\n",
            "    display_name: 'line_dry_in_the_shade'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 27\n",
            "    name: 'line_dry'\n",
            "    display_name: 'line_dry'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 28\n",
            "    name: 'no_steam'\n",
            "    display_name: 'no_steam'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 29\n",
            "    name: 'non_chlorine_bleach_when_needed'\n",
            "    display_name: 'non_chlorine_bleach_when_needed'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 30\n",
            "    name: 'professional_cleaning'\n",
            "    display_name: 'professional_cleaning'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 31\n",
            "    name: 'professional_wet_cleaning'\n",
            "    display_name: 'professional_wet_cleaning'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 32\n",
            "    name: 'tumble_drying_low_temps'\n",
            "    display_name: 'tumble_drying_low_temps'\n",
            " }\n",
            " \n",
            "  item {\n",
            "    id: 33\n",
            "    name: 'tumble_drying_normal'\n",
            "    display_name: 'tumble_drying_normal'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 34\n",
            "    name: 'tumble_drying'\n",
            "    display_name: 'tumble_drying'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 35\n",
            "    name: 'very_gentle_cleaning_with_hydrocarbon_solvents'\n",
            "    display_name: 'very_gentle_cleaning_with_hydrocarbon_solvents'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 36\n",
            "    name: 'very_gentle_cleaning_with_pce'\n",
            "    display_name: 'very_gentle_cleaning_with_pce'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 37\n",
            "    name: 'very_gentle_wet_cleaning'\n",
            "    display_name: 'very_gentle_wet_cleaning'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 38\n",
            "    name: 'wash_at_or_below_30_mild_fine_wash'\n",
            "    display_name: 'wash_at_or_below_30_mild_fine_wash'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 39\n",
            "    name: 'wash_at_or_below_30_very_mild_fine_wash'\n",
            "    display_name: 'wash_at_or_below_30_very_mild_fine_wash'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 40\n",
            "    name: 'wash_at_or_below_30'\n",
            "    display_name: 'wash_at_or_below_30'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 41\n",
            "    name: 'wash_at_or_below_40_mild_fine_wash'\n",
            "    display_name: 'wash_at_or_below_40_mild_fine_wash'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 42\n",
            "    name: 'wash_at_or_below_40_very_mild_fine_wash'\n",
            "    display_name: 'wash_at_or_below_40_very_mild_fine_wash'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 43\n",
            "    name: 'wash_at_or_below_40'\n",
            "    display_name: 'wash_at_or_below_40'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 44\n",
            "    name: 'wash_at_or_below_50_mild_fine_wash'\n",
            "    display_name: 'wash_at_or_below_50_mild_fine_wash'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 45\n",
            "    name: 'wash_at_or_below_50'\n",
            "    display_name: 'wash_at_or_below_50'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 46\n",
            "    name: 'wash_at_or_below_60_mild_fine_wash'\n",
            "    display_name: 'wash_at_or_below_60_mild_fine_wash'\n",
            " }\n",
            "\n",
            "   item {\n",
            "    id: 47\n",
            "    name: 'wash_at_or_below_60'\n",
            "    display_name: 'wash_at_or_below_60'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 48\n",
            "    name: 'wash_at_or_below_70'\n",
            "    display_name: 'wash_at_or_below_70'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 49\n",
            "    name: 'wash_at_or_below_90'\n",
            "    display_name: 'wash_at_or_below_90'\n",
            " }\n",
            "\n",
            "  item {\n",
            "    id: 50\n",
            "    name: 'washing_symbol'\n",
            "    display_name: 'washing_symbol'\n",
            " }"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_tyvKnBP6qD",
        "colab_type": "text"
      },
      "source": [
        "## Downloading and Preparing Tensorflow model\n",
        "1. Cloning [Tensorflow models](https://github.com/tensorflow/models.git). This repo contains object detection API. \n",
        "2. Compiling the protos and adding folders to the os environment.\n",
        "3. Testing the model builder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIxz1GqJQA3f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8c87512-6666-4b6f-aec3-afc223b9d3ae"
      },
      "source": [
        "%cd /content/CareLabelsData/\n",
        "!git clone --q https://github.com/tensorflow/models.git"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/CareLabelsData\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjcAhsxRQ5N1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "083ad3a4-6b94-4112-9370-1fc7aaf871e8"
      },
      "source": [
        "%cd /content/CareLabelsData/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "os.environ['PYTHONPATH'] += ':/content/CareLabelsData/models/research/:/content/CareLabelsData/models/research/slim/'"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/CareLabelsData/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bMNsrwTSJi2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "96730a4e-3a03-4c69-921b-059de580e993"
      },
      "source": [
        "!pip3 install tf_slim\n",
        "!python3 object_detection/builders/model_builder_test.py"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf_slim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\r\u001b[K     |█                               | 10kB 17.2MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 5.0MB/s eta 0:00:01\r\u001b[K     |██▉                             | 30kB 5.1MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40kB 5.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 51kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 61kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 71kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 81kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 92kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 102kB 7.2MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 112kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 122kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 133kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 143kB 7.2MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 153kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 163kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 174kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 184kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 194kB 7.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 204kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 215kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 225kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 235kB 7.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 245kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 256kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 266kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 276kB 7.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 286kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 296kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 307kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 317kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 327kB 7.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 337kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 348kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 358kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf_slim) (0.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.2.2->tf_slim) (1.15.0)\n",
            "Installing collected packages: tf-slim\n",
            "Successfully installed tf-slim-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9C3L_r4Pi6m",
        "colab_type": "text"
      },
      "source": [
        "## Generating Tf record\n",
        "- Generating TFRecords files for training and testing csv's.\n",
        "- Tensorflow accepts the data as tfrecords which is a binary file that run fast with low memory usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK2unk-9LB_E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "dc868d83-5705-4f5c-ad3f-050a92fad2a2"
      },
      "source": [
        "from object_detection.utils import dataset_util\n",
        "%cd /content/CareLabelsData/models/\n",
        "\n",
        "DATA_BASE_PATH = '/content/CareLabelsData/data/'\n",
        "image_dir = '/content/CareLabelsData/data/images'\n",
        "\n",
        "def class_text_to_int(row_label):\n",
        "\t\tif row_label == 'bleaching_with_chlorine_allowed':\n",
        "\t\t\t\treturn 1\n",
        "\t\telif row_label == 'chlorine_and_non_chlorine_bleach':\n",
        "\t\t\t\treturn 2\n",
        "\t\telif row_label == 'do_not_bleach':\n",
        "\t\t\t\treturn 3\n",
        "\t\telif row_label == 'do_not_dry_clean':\n",
        "\t\t\t\treturn 4\n",
        "\t\telif row_label == 'do_not_iron':\n",
        "\t\t\t\treturn 5\n",
        "\t\telif row_label == 'do_not_tumble_drying':\n",
        "\t\t\t\treturn 6\n",
        "\t\telif row_label == 'do_not_wash':\n",
        "\t\t\t\treturn 7\n",
        "\t\telif row_label == 'do_not_wet_clean':\n",
        "\t\t\t\treturn 8\n",
        "\t\telif row_label == 'drip_dry_in_the_shade':\n",
        "\t\t\t\treturn 9\n",
        "\t\telif row_label == 'drip_dry':\n",
        "\t\t\t\treturn 10\n",
        "\t\telif row_label == 'dry_clean_any_solvent':\n",
        "\t\t\t\treturn 11\n",
        "\t\telif row_label == 'dry_clean_hydrocarbon_solvent_only_hcs':\n",
        "\t\t\t\treturn 12\n",
        "\t\telif row_label == 'dry_clean_tetrachloroethylene_pce_only':\n",
        "\t\t\t\treturn 13\n",
        "\t\telif row_label == 'dry_flat_in_the_shade':\n",
        "\t\t\t\treturn 14\n",
        "\t\telif row_label == 'dry_flat':\n",
        "\t\t\t\treturn 15\n",
        "\t\telif row_label == 'dry_in_the_shade':\n",
        "\t\t\t\treturn 16\n",
        "\t\telif row_label == 'drying_symbol':\n",
        "\t\t\t\treturn 17\n",
        "\t\telif row_label == 'gentle_cleaning_with_hydrocarbon_solvents':\n",
        "\t\t\t\treturn 18\n",
        "\t\telif row_label == 'gentle_cleaning_with_pce':\n",
        "\t\t\t\treturn 19\n",
        "\t\telif row_label == 'gentle_wet_cleaning':\n",
        "\t\t\t\treturn 20\n",
        "\t\telif row_label == 'hand_wash':\n",
        "\t\t\t\treturn 21\n",
        "\t\telif row_label == 'ironing_at_high_temp':\n",
        "\t\t\t\treturn 22\n",
        "\t\telif row_label == 'ironing_at_low_temp':\n",
        "\t\t\t\treturn 23\n",
        "\t\telif row_label == 'ironing_at_med_temp':\n",
        "\t\t\t\treturn 24\n",
        "\t\telif row_label == 'ironing':\n",
        "\t\t\t\treturn 25\n",
        "\t\telif row_label == 'line_dry_in_the_shade':\n",
        "\t\t\t\treturn 26\n",
        "\t\telif row_label == 'line_dry':\n",
        "\t\t\t\treturn 27\n",
        "\t\telif row_label == 'no_steam':\n",
        "\t\t\t\treturn 28\n",
        "\t\telif row_label == 'non_chlorine_bleach_when_needed':\n",
        "\t\t\t\treturn 29\n",
        "\t\telif row_label == 'professional_cleaning':\n",
        "\t\t\t\treturn 30\n",
        "\t\telif row_label == 'professional_wet_cleaning':\n",
        "\t\t\t\treturn 31\n",
        "\t\telif row_label == 'tumble_drying_low_temps':\n",
        "\t\t\t\treturn 32\n",
        "\t\telif row_label == 'tumble_drying_normal':\n",
        "\t\t\t\treturn 33\n",
        "\t\telif row_label == 'tumble_drying':\n",
        "\t\t\t\treturn 34\n",
        "\t\telif row_label == 'very_gentle_cleaning_with_hydrocarbon_solvents':\n",
        "\t\t\t\treturn 35\n",
        "\t\telif row_label == 'very_gentle_cleaning_with_pce':\n",
        "\t\t\t\treturn 36\n",
        "\t\telif row_label == 'very_gentle_wet_cleaning':\n",
        "\t\t\t\treturn 37\n",
        "\t\telif row_label == 'wash_at_or_below_30_mild_fine_wash':\n",
        "\t\t\t\treturn 38\n",
        "\t\telif row_label == 'wash_at_or_below_30_very_mild_fine_wash':\n",
        "\t\t\t\treturn 39\n",
        "\t\telif row_label == 'wash_at_or_below_30':\n",
        "\t\t\t\treturn 40\n",
        "\t\telif row_label == 'wash_at_or_below_40_mild_fine_wash':\n",
        "\t\t\t\treturn 41\n",
        "\t\telif row_label == 'wash_at_or_below_40_very_mild_fine_wash':\n",
        "\t\t\t\treturn 42\n",
        "\t\telif row_label == 'wash_at_or_below_40':\n",
        "\t\t\t\treturn 43\n",
        "\t\telif row_label == 'wash_at_or_below_50_mild_fine_wash':\n",
        "\t\t\t\treturn 44\n",
        "\t\telif row_label == 'wash_at_or_below_50':\n",
        "\t\t\t\treturn 45\n",
        "\t\telif row_label == 'wash_at_or_below_60_mild_fine_wash':\n",
        "\t\t\t\treturn 46\n",
        "\t\telif row_label == 'wash_at_or_below_60':\n",
        "\t\t\t\treturn 47\n",
        "\t\telif row_label == 'wash_at_or_below_70':\n",
        "\t\t\t\treturn 48\n",
        "\t\telif row_label == 'wash_at_or_below_90':\n",
        "\t\t\t\treturn 49\n",
        "\t\telif row_label == 'washing_symbol':\n",
        "\t\t\t\treturn 50\t\t\t\n",
        "\t\telse:\n",
        "\t\t\t\t0\n",
        "\n",
        "def split(df, group):\n",
        "\t\tdata = namedtuple('data', ['filename', 'object'])\n",
        "\t\tgb = df.groupby(group)\n",
        "\t\treturn [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "\t\twith tf.io.gfile.GFile(os.path.join(path, '{}'.format(os.path.splitext(group.filename)[0])), 'rb') as fid:\n",
        "\t\t\t\tencoded_jpg = fid.read()\n",
        "\t\tencoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "\t\timage = Image.open(encoded_jpg_io)\n",
        "\t\twidth, height = image.size\n",
        "\n",
        "\t\tfilename = group.filename.encode('utf8')\n",
        "\t\txmins = []\n",
        "\t\txmaxs = []\n",
        "\t\tymins = []\n",
        "\t\tymaxs = []\n",
        "\t\tclasses_text = []\n",
        "\t\tclasses = []\n",
        "\n",
        "\t\tfor index, row in group.object.iterrows():\n",
        "\t\t\t\txmins.append(row['xmin'] / width)\n",
        "\t\t\t\txmaxs.append(row['xmax'] / width)\n",
        "\t\t\t\tymins.append(row['ymin'] / height)\n",
        "\t\t\t\tymaxs.append(row['ymax'] / height)\n",
        "\t\t\t\tclasses_text.append(row['class'].encode('utf8'))\n",
        "\t\t\t\tclasses.append(class_text_to_int(row['class']))\n",
        "\n",
        "\t\ttf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "\t\t\t\t'image/height': dataset_util.int64_feature(height),\n",
        "\t\t\t\t'image/width': dataset_util.int64_feature(width),\n",
        "\t\t\t\t'image/filename': dataset_util.bytes_feature(os.path.splitext(filename)[0]),\n",
        "\t\t\t\t'image/source_id': dataset_util.bytes_feature(os.path.splitext(filename)[0]),\n",
        "\t\t\t\t'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "\t\t\t\t'image/format': dataset_util.bytes_feature(b'jpg'),\n",
        "\t\t\t\t'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "\t\t\t\t'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "\t\t\t\t'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "\t\t\t\t'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "\t\t\t\t'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "\t\t\t\t'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "\t\t}))\n",
        "\t\treturn tf_example\n",
        "\n",
        "for csv in ['train_labels', 'test_labels']:\n",
        "  writer = tf.io.TFRecordWriter(DATA_BASE_PATH + csv + '.record')\n",
        "  path = os.path.join(image_dir)\n",
        "  examples = pd.read_csv(DATA_BASE_PATH + csv + '.csv')\n",
        "  grouped = split(examples, 'filename')\n",
        "  for group in grouped:\n",
        "      tf_example = create_tf_example(group, path)\n",
        "      writer.write(tf_example.SerializeToString())\n",
        "    \n",
        "  writer.close()\n",
        "  output_path = os.path.join(os.getcwd(), DATA_BASE_PATH + csv + '.record')\n",
        "  print('Successfully created the TFRecords: {}'.format(DATA_BASE_PATH +csv + '.record'))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/CareLabelsData/models\n",
            "Successfully created the TFRecords: /content/CareLabelsData/data/train_labels.record\n",
            "Successfully created the TFRecords: /content/CareLabelsData/data/test_labels.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1zRJducWs-X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "adf3e76f-722c-4948-c96e-eaba5c493aa5"
      },
      "source": [
        "!ls -lX /content/CareLabelsData/data/"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 66792\n",
            "drwxr-xr-x 2 root root     4096 Sep 13 14:45 images\n",
            "drwxr-xr-x 2 root root     4096 Sep 13 14:45 test_labels\n",
            "drwxr-xr-x 2 root root     4096 Sep 13 14:45 train_labels\n",
            "-rw-r--r-- 1 root root      929 Sep 13 14:46 test_labels.csv\n",
            "-rw-r--r-- 1 root root    32735 Sep 13 14:46 train_labels.csv\n",
            "-rw-r--r-- 1 root root     5092 Sep 13 14:46 label_map.pbtxt\n",
            "-rw-r--r-- 1 root root  2006675 Sep 13 14:47 test_labels.record\n",
            "-rw-r--r-- 1 root root 66329466 Sep 13 14:47 train_labels.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMckMSJqFMyc",
        "colab_type": "text"
      },
      "source": [
        "## Downloading the Base Model\n",
        "1. Downloading the selected model and extracting its content.\n",
        "2. Creating a directory to save model while training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvN9Cw65FQzB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2963e651-94b6-488c-fcdd-4163f799a4c5"
      },
      "source": [
        "%cd /content/CareLabelsData/models/research\n",
        "\n",
        "MODEL = MODELS_CONFIG[selected_model]['model_name']\n",
        "\n",
        "pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n",
        "\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "\n",
        "fine_tune_dir = '/content/CareLabelsData/models/research/pretrained_model'\n",
        "\n",
        "#checks if already downloaded\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(fine_tune_dir)):\n",
        "    shutil.rmtree(fine_tune_dir)\n",
        "os.rename(MODEL, fine_tune_dir)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/CareLabelsData/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbjXKVMmFk47",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fb94a08f-39b0-4237-86ca-0b8a221c493b"
      },
      "source": [
        "!echo {fine_tune_dir}\n",
        "!ls -alh {fine_tune_dir}"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/CareLabelsData/models/research/pretrained_model\n",
            "total 135M\n",
            "drwxr-xr-x  3 345018 89939 4.0K Mar 30  2018 .\n",
            "drwxr-xr-x 24 root   root  4.0K Sep 13 14:47 ..\n",
            "-rw-r--r--  1 345018 89939   77 Mar 30  2018 checkpoint\n",
            "-rw-r--r--  1 345018 89939  67M Mar 30  2018 frozen_inference_graph.pb\n",
            "-rw-r--r--  1 345018 89939  65M Mar 30  2018 model.ckpt.data-00000-of-00001\n",
            "-rw-r--r--  1 345018 89939  15K Mar 30  2018 model.ckpt.index\n",
            "-rw-r--r--  1 345018 89939 3.4M Mar 30  2018 model.ckpt.meta\n",
            "-rw-r--r--  1 345018 89939 4.2K Mar 30  2018 pipeline.config\n",
            "drwxr-xr-x  3 345018 89939 4.0K Mar 30  2018 saved_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnjQgJZiGAcA",
        "colab_type": "text"
      },
      "source": [
        "## Configuring the Training Pipeline\n",
        "1. Adding the path for the TFRecords files and pbtxt,batch_size,num_steps,num_classes to the configuration file.\n",
        "2. Adding some Image augmentation.\n",
        "3. Creating a directory to save the model at each checkpoint while training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az14XVo31Ujp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "9641245c-752f-4b6f-bd85-0ee6fb9621f3"
      },
      "source": [
        "CONFIG_BASE = \"/content/CareLabelsData/models/research/object_detection/samples/configs/\"\n",
        "\n",
        "model_pipline = os.path.join(CONFIG_BASE, pipeline_file)\n",
        "model_pipline"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/CareLabelsData/models/research/object_detection/samples/configs/ssd_mobilenet_v2_coco.config'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kfsl5CsDGY3-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e7dec500-ac64-44f0-af1c-3802a3dd1119"
      },
      "source": [
        "%%writefile {model_pipline}\n",
        "model {\n",
        "  ssd {\n",
        "    num_classes: 50 # number of classes\n",
        "    box_coder {\n",
        "      faster_rcnn_box_coder {\n",
        "        y_scale: 10.0\n",
        "        x_scale: 10.0\n",
        "        height_scale: 5.0\n",
        "        width_scale: 5.0\n",
        "      }\n",
        "    }\n",
        "    matcher {\n",
        "      argmax_matcher {\n",
        "        matched_threshold: 0.5\n",
        "        unmatched_threshold: 0.5\n",
        "        ignore_thresholds: false\n",
        "        negatives_lower_than_unmatched: true\n",
        "        force_match_for_each_row: true\n",
        "      }\n",
        "    }\n",
        "    similarity_calculator {\n",
        "      iou_similarity {\n",
        "      }\n",
        "    }\n",
        "    anchor_generator {\n",
        "      ssd_anchor_generator {\n",
        "        num_layers: 6\n",
        "        min_scale: 0.2\n",
        "        max_scale: 0.95\n",
        "        aspect_ratios: 1.0\n",
        "        aspect_ratios: 2.0\n",
        "        aspect_ratios: 0.5\n",
        "        aspect_ratios: 3.0\n",
        "        aspect_ratios: 0.3333\n",
        "      }\n",
        "    }\n",
        "    # all images will be resized to height and width parametrs\n",
        "    image_resizer { \n",
        "      fixed_shape_resizer {\n",
        "        height: 300\n",
        "        width: 300\n",
        "      }\n",
        "    }\n",
        "    box_predictor {\n",
        "      convolutional_box_predictor {\n",
        "        min_depth: 0\n",
        "        max_depth: 0\n",
        "        num_layers_before_predictor: 0\n",
        "        use_dropout: true # to counter over fitting\n",
        "        dropout_keep_probability: 0.8\n",
        "        kernel_size: 1\n",
        "        box_code_size: 4\n",
        "        apply_sigmoid_to_scores: false\n",
        "        conv_hyperparams {\n",
        "          activation: RELU_6,\n",
        "          regularizer {\n",
        "            l2_regularizer {\n",
        "            weight: 0.001 # higher regularizition to counter overfitting\n",
        "          }\n",
        "          }\n",
        "          initializer {\n",
        "            truncated_normal_initializer {\n",
        "              stddev: 0.03\n",
        "              mean: 0.0\n",
        "            }\n",
        "          }\n",
        "          batch_norm {\n",
        "            train: true,\n",
        "            scale: true,\n",
        "            center: true,\n",
        "            decay: 0.9997,\n",
        "            epsilon: 0.001,\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    feature_extractor {\n",
        "      type: 'ssd_mobilenet_v2'\n",
        "      min_depth: 16\n",
        "      depth_multiplier: 1.0\n",
        "      conv_hyperparams {\n",
        "        activation: RELU_6,\n",
        "        regularizer {\n",
        "          l2_regularizer {\n",
        "            weight: 0.001 # higher regularizition to counter overfitting\n",
        "          }\n",
        "        }\n",
        "        initializer {\n",
        "          truncated_normal_initializer {\n",
        "            stddev: 0.03\n",
        "            mean: 0.0\n",
        "          }\n",
        "        }\n",
        "        batch_norm {\n",
        "          train: true,\n",
        "          scale: true,\n",
        "          center: true,\n",
        "          decay: 0.9997,\n",
        "          epsilon: 0.001,\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    loss {\n",
        "      classification_loss {\n",
        "        weighted_sigmoid {\n",
        "        }\n",
        "      }\n",
        "      localization_loss {\n",
        "        weighted_smooth_l1 {\n",
        "        }\n",
        "      }\n",
        "      hard_example_miner {\n",
        "        num_hard_examples: 3000 \n",
        "        iou_threshold: 0.95\n",
        "        loss_type: CLASSIFICATION\n",
        "        max_negatives_per_positive: 3\n",
        "        min_negatives_per_image: 3\n",
        "      }\n",
        "      classification_weight: 1.0\n",
        "      localization_weight: 1.0\n",
        "    }\n",
        "    normalize_loss_by_num_matches: true\n",
        "    post_processing {\n",
        "      batch_non_max_suppression {\n",
        "        score_threshold: 1e-8\n",
        "        iou_threshold: 0.6\n",
        "        \n",
        "        #there are some images with more than one up to 3.\n",
        "        #5 for future proof\n",
        "        max_detections_per_class: 5\n",
        "        #probably overkill, max 12 labels in one image \n",
        "        max_total_detections: 16\n",
        "      }\n",
        "      score_converter: SIGMOID\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "train_config: {\n",
        "  batch_size: 16 # training batch size\n",
        "  optimizer {\n",
        "    rms_prop_optimizer: {\n",
        "      learning_rate: {\n",
        "        exponential_decay_learning_rate {\n",
        "          initial_learning_rate: 0.003\n",
        "          decay_steps: 800720\n",
        "          decay_factor: 0.95\n",
        "        }\n",
        "      }\n",
        "      momentum_optimizer_value: 0.9\n",
        "      decay: 0.9\n",
        "      epsilon: 1.0\n",
        "    }\n",
        "  }\n",
        "\n",
        "  #the path to the pretrained model. \n",
        "  fine_tune_checkpoint: \"/content/CareLabelsData/models/research/pretrained_model/model.ckpt\"\n",
        "  fine_tune_checkpoint_type:  \"detection\"\n",
        "  num_steps: 20000 \n",
        "  \n",
        "\n",
        "  # data augmentaion will help the model generalize but training time will increase greatly  \n",
        "  data_augmentation_options {\n",
        "    random_horizontal_flip {\n",
        "    }\n",
        "  }\n",
        "  data_augmentation_options {\n",
        "    random_adjust_contrast {\n",
        "    }\n",
        "  }\n",
        "  data_augmentation_options {\n",
        "    ssd_random_crop {\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "train_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/CareLabelsData/data/train_labels.record\"    #path to the training TFRecord\n",
        "  }\n",
        "  label_map_path: \"/content/CareLabelsData/data/label_map.pbtxt\"  #path to the label map \n",
        "}\n",
        "\n",
        "eval_config: {\n",
        "  num_examples: 3  # the number of images in your \"testing\" data\n",
        "  num_visualizations: 3  # the number of images to disply in Tensorboard\n",
        "  # Remove the below line to evaluate indefinitely.\n",
        "  #max_evals: 10\n",
        "}\n",
        "\n",
        "eval_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/CareLabelsData/data/test_labels.record\"    #path to the testing TFRecord\n",
        "  }\n",
        "  label_map_path: \"/content/CareLabelsData/data/label_map.pbtxt\"  #path to the label map \n",
        "  shuffle: false\n",
        "  num_readers: 1\n",
        "}"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/CareLabelsData/models/research/object_detection/samples/configs/ssd_mobilenet_v2_coco.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuXXZLVEG8sO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir = 'training/'# dir where model will be saved at each checkpoint\n",
        "\n",
        "# remove content in output model directory to fresh start.\n",
        "!rm -rf {model_dir}\n",
        "os.makedirs(model_dir, exist_ok=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vAGvftxHu8K",
        "colab_type": "text"
      },
      "source": [
        "## Tensorboard\n",
        "1. Downlaoding and Unzipping\n",
        "2. Creating a link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2ucxlc5HxHL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "a45e78a7-bdfb-4a9a-b0a2-9d766faf4cb0"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-13 14:47:18--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.236.74.205, 54.163.152.154, 34.227.164.168, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.236.74.205|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  13.2MB/s    in 1.0s    \n",
            "\n",
            "2020-09-13 14:47:19 (13.2 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w9ufxr7IAdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_DIR = model_dir\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idsi9zyNIIsr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2f9a0191-9bd8-4ca5-ce9e-9144b4570045"
      },
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://001a457f7bb3.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuJcAPZFIfu7",
        "colab_type": "text"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnKt6g0_IgOe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dbb3ea8d-b9dc-4741-936c-d1db91c02abb"
      },
      "source": [
        "!python3 /content/CareLabelsData/models/research/object_detection/model_main.py \\\n",
        "    --pipeline_config_path={model_pipline}\\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\n",
            "W0913 14:47:30.729691 139939402770304 model_lib.py:771] Forced number of epochs for all eval validations to be 1.\n",
            "INFO:tensorflow:Maybe overwriting train_steps: None\n",
            "I0913 14:47:30.729938 139939402770304 config_util.py:552] Maybe overwriting train_steps: None\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I0913 14:47:30.730077 139939402770304 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: 1\n",
            "I0913 14:47:30.730259 139939402770304 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: 1\n",
            "INFO:tensorflow:Maybe overwriting eval_num_epochs: 1\n",
            "I0913 14:47:30.730406 139939402770304 config_util.py:552] Maybe overwriting eval_num_epochs: 1\n",
            "WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "W0913 14:47:30.730582 139939402770304 model_lib.py:787] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "INFO:tensorflow:create_estimator_and_inputs: use_tpu False, export_to_tpu None\n",
            "I0913 14:47:30.730841 139939402770304 model_lib.py:822] create_estimator_and_inputs: use_tpu False, export_to_tpu None\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f45c2bc82e8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "I0913 14:47:30.731376 139939402770304 estimator.py:212] Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f45c2bc82e8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f45c2bc0d90>) includes params argument, but params are not passed to Estimator.\n",
            "W0913 14:47:30.731665 139939402770304 model_fn.py:630] Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f45c2bc0d90>) includes params argument, but params are not passed to Estimator.\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "I0913 14:47:30.732532 139939402770304 estimator_training.py:186] Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "I0913 14:47:30.732758 139939402770304 training.py:612] Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "I0913 14:47:30.733057 139939402770304 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0913 14:47:30.745339 139939402770304 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W0913 14:47:30.801717 139939402770304 dataset_builder.py:83] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /content/CareLabelsData/models/research/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0913 14:47:30.808699 139939402770304 deprecation.py:323] From /content/CareLabelsData/models/research/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "WARNING:tensorflow:From /content/CareLabelsData/models/research/object_detection/builders/dataset_builder.py:175: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0913 14:47:30.832545 139939402770304 deprecation.py:323] From /content/CareLabelsData/models/research/object_detection/builders/dataset_builder.py:175: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f45c2bc8e48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "W0913 14:47:30.870889 139939402770304 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f45c2bc8e48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <function train_input.<locals>.transform_and_pad_input_data_fn at 0x7f45e231bd08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "W0913 14:47:31.108281 139939402770304 ag_logging.py:146] Entity <function train_input.<locals>.transform_and_pad_input_data_fn at 0x7f45e231bd08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From /content/CareLabelsData/models/research/object_detection/inputs.py:80: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W0913 14:47:31.115947 139939402770304 deprecation.py:323] From /content/CareLabelsData/models/research/object_detection/inputs.py:80: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /content/CareLabelsData/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0913 14:47:31.125045 139939402770304 deprecation.py:323] From /content/CareLabelsData/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/CareLabelsData/models/research/object_detection/core/preprocessor.py:199: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W0913 14:47:31.254196 139939402770304 deprecation.py:323] From /content/CareLabelsData/models/research/object_detection/core/preprocessor.py:199: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "WARNING:tensorflow:From /content/CareLabelsData/models/research/object_detection/inputs.py:261: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0913 14:47:32.294361 139939402770304 deprecation.py:323] From /content/CareLabelsData/models/research/object_detection/inputs.py:261: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0913 14:47:32.864772 139939402770304 estimator.py:1148] Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W0913 14:47:33.189655 139939402770304 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0913 14:47:36.484381 139939402770304 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0913 14:47:36.673050 139939402770304 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0913 14:47:36.722643 139939402770304 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0913 14:47:36.773765 139939402770304 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0913 14:47:36.821051 139939402770304 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0913 14:47:36.870172 139939402770304 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0913 14:47:43.072958 139939402770304 deprecation.py:506] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPN8liiQc7Ue",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Exporting trained model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upwUdom0lTub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_directory = '/content/CareLabelsData/models/research/fine_tuned_model'# exported model location\n",
        "\n",
        "lst = os.listdir(model_dir)\n",
        "lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n",
        "steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n",
        "last_model = lst[steps.argmax()].replace('.meta', '')\n",
        "last_model_path = os.path.join(model_dir, last_model)\n",
        "print(last_model_path)\n",
        "\n",
        "!python /content/CareLabelsData/models/research/object_detection/export_inference_graph.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path={model_pipline} \\\n",
        "    --output_directory={output_directory} \\\n",
        "    --trained_checkpoint_prefix={last_model_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuxDnGPM_JPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(output_directory + '/frozen_inference_graph.pb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTkkaGq5BpYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(DATA_BASE_PATH + '/label_map.pbtxt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD6baVVZXZsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python /content/CareLabelsData/models/research/object_detection/export_tflite_ssd_graph.py — pipeline_config_path /content/CareLabelsData/models/research/pretrained_model/pipeline.config — trained_checkpoint_prefix /content/CareLabelsData/models/research/training/model.ckpt-785 — output_directory \"/content/object_detection_demo/\" — add_postprocessing_op True — max_detections 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWKrkgMGcx79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python /content/CareLabelsData/models/research/object_detection/export_tflite_ssd_graph.py \\\n",
        "--pipeline_config_path=/content/CareLabelsData/models/research/fine_tuned_model/pipeline.config \\\n",
        "--trained_checkpoint_prefix=\"/content/CareLabelsData/models/research/training/model.ckpt-785\" \\\n",
        "--output_directory=/content/CareLabelsData/dddd/ \\\n",
        "--add_postprocessing_op=true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6LEZEOLZ2r9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install toco"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_AZcHopXbzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!toco \\\n",
        " --graph_def_file=\"/content/CareLabelsData/tflite_graph.pb\" \\\n",
        " --output_file=\"/content/CareLabelsData/detect.tflite\" \\\n",
        " --input_shapes=1,300,300,3 \\\n",
        " --input_arrays=normalized_input_image_tensor \\\n",
        " --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\n",
        " --inference_type=FLOAT \\\n",
        " --allow_custom_ops"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}